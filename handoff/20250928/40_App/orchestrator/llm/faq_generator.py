"""
GPT-4 based FAQ content generator

Replaces hardcoded FAQ template with dynamic AI-generated content
based on user questions and repository context.
"""
import os
import logging
from typing import Optional
from openai import OpenAI

logger = logging.getLogger(__name__)

def _get_openai_client():
    """Get or create OpenAI client lazily"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY environment variable not set")
    return OpenAI(api_key=api_key)

SYSTEM_PROMPT = """You are a technical documentation expert helping to generate FAQ content for the MorningAI platform.

MorningAI is a multi-tenant SaaS platform that provides:
- Autonomous agent system for code generation
- FAQ generation and documentation management
- Multi-platform integration (Telegram, LINE, Messenger)
- Real-time task orchestration with Redis Queue
- Vector memory storage with pgvector/Supabase

Technology Stack:
- Frontend: React, Vite, TailwindCSS
- Backend: Python, Flask, Gunicorn with multi-worker support
- Database: PostgreSQL (Supabase) with Row Level Security
- Queue: Redis Queue (RQ) with worker heartbeat monitoring
- Orchestration: LangGraph for agent workflows
- AI: OpenAI GPT-4 for content generation
- Deployment: Render.com with CI/CD

When generating FAQ content:
1. Be concise and technically accurate
2. Include code examples when relevant
3. Reference actual file paths and architecture
4. Maintain professional tone
5. Include setup/configuration instructions if asked
6. Add troubleshooting sections for common issues
7. Use Markdown formatting

Always end with:
---
Generated by MorningAI Orchestrator using GPT-4
"""

def generate_faq_content(
    question: str,
    trace_id: str,
    repo: str = "RC918/morningai",
    model: str = "gpt-4-turbo-preview"
) -> str:
    """
    Generate FAQ content using GPT-4 based on user question
    
    Args:
        question: User's FAQ question or topic
        trace_id: Unique trace identifier for this task
        repo: GitHub repository (owner/repo format)
        model: OpenAI model to use (default: gpt-4-turbo-preview)
    
    Returns:
        str: Generated FAQ content in Markdown format
    
    Raises:
        Exception: If OpenAI API call fails
    """
    logger.info(f"Generating FAQ content with GPT-4", extra={
        "operation": "generate_faq",
        "trace_id": trace_id,
        "question": question[:50],
        "model": model
    })
    
    user_prompt = f"""Generate comprehensive FAQ content for the following question/topic:

**Question**: {question}

**Context**:
- Repository: {repo}
- This will be added to docs/FAQ.md in the repository
- The FAQ should help developers understand and use MorningAI

Please generate:
1. A clear title (use # heading)
2. Comprehensive explanation of the topic
3. Code examples if applicable
4. Related documentation links
5. Common troubleshooting tips if relevant

Keep it professional, technically accurate, and well-formatted in Markdown.
"""
    
    try:
        client = _get_openai_client()
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7,
            max_tokens=2000,
            top_p=0.9,
            frequency_penalty=0.3,
            presence_penalty=0.3
        )
        
        generated_content = response.choices[0].message.content.strip()
        
        faq_content = f"""{generated_content}

---

**Metadata**:
- Task: {question}
- Trace ID: `{trace_id}`
- Generated by: MorningAI Orchestrator using {model}
- Repository: {repo}
"""
        
        logger.info(f"FAQ content generated successfully", extra={
            "operation": "generate_faq",
            "trace_id": trace_id,
            "content_length": len(faq_content),
            "model": model,
            "tokens_used": response.usage.total_tokens if response.usage else None
        })
        
        return faq_content
        
    except Exception as e:
        logger.error(f"Failed to generate FAQ with GPT-4: {e}", extra={
            "operation": "generate_faq",
            "trace_id": trace_id,
            "error": str(e)
        })
        
        logger.warning(f"Falling back to template-based FAQ generation", extra={
            "operation": "generate_faq",
            "trace_id": trace_id
        })
        
        return generate_fallback_faq(question, trace_id, repo)

def generate_fallback_faq(
    question: str,
    trace_id: str,
    repo: str = "RC918/morningai"
) -> str:
    """
    Fallback FAQ generator when GPT-4 is unavailable
    Uses enhanced template with better structure than original hardcoded version
    
    Args:
        question: User's FAQ question
        trace_id: Trace identifier
        repo: Repository name
    
    Returns:
        str: Template-based FAQ content
    """
    return f"""# Frequently Asked Questions (FAQ)

{question}


MorningAI is a comprehensive multi-tenant SaaS platform for autonomous agent-based code generation and documentation management.


- **Autonomous Agent System**: GPT-4 powered agents for automated code generation and PR creation
- **Multi-tenant Architecture**: Row Level Security (RLS) for complete tenant data isolation
- **Real-time Task Queue**: Redis Queue with worker heartbeat monitoring for reliable task execution
- **Vector Memory**: pgvector-based memory storage for context-aware agents
- **Multi-platform Integration**: Support for Telegram, LINE, Messenger via platform_bindings


**Frontend**:
- React with Vite
- TailwindCSS for styling
- Modern ES6+ JavaScript

**Backend**:
- Python 3.12
- Flask web framework
- Gunicorn with 4 workers for production
- Row Level Security (RLS) for tenant isolation

**Infrastructure**:
- PostgreSQL (Supabase) for data persistence
- Redis for task queue and caching
- Redis Queue (RQ) for async job processing
- Sentry for error tracking and monitoring

**AI & Orchestration**:
- OpenAI GPT-4 for content generation
- LangGraph for agent workflow orchestration
- pgvector for semantic search and memory


Please refer to our documentation:
- [README](../README.md) - Installation and setup
- [CONTRIBUTING](../CONTRIBUTING.md) - Contribution guidelines
- [RLS Implementation Guide](../docs/RLS_IMPLEMENTATION_GUIDE.md) - Database security


```
┌─────────────┐      ┌──────────────┐      ┌─────────────┐
│   Frontend  │─────▶│  API Backend │─────▶│   Supabase  │
│   (React)   │      │   (Flask)    │      │ (PostgreSQL)│
└─────────────┘      └──────────────┘      └─────────────┘
                            │
                            ├───▶ Redis Queue
                            │         │
                            │         ▼
                            │    Orchestrator Worker
                            │    (LangGraph + GPT-4)
                            │         │
                            └─────────┘
```


**Start Development Server**:
```bash
cd handoff/20250928/40_App/api-backend/src
gunicorn -c ../gunicorn.conf.py main:app
```

**Run Tests**:
```bash
cd handoff/20250928/40_App/api-backend
pytest tests/ -v
```

**Check RLS Policies**:
```sql
SELECT tablename, rowsecurity FROM pg_tables 
WHERE schemaname = 'public' AND rowsecurity = true;
```

---

**Metadata**:
- Task: {question}
- Trace ID: `{trace_id}`
- Generated by: MorningAI Orchestrator (Fallback Template)
- Repository: {repo}
"""

_faq_cache = {}

def get_cached_or_generate(
    question: str,
    trace_id: str,
    repo: str = "RC918/morningai",
    use_cache: bool = True
) -> str:
    """
    Get FAQ content from cache or generate new content
    
    Args:
        question: FAQ question
        trace_id: Trace identifier
        repo: Repository name
        use_cache: Whether to use cached responses (default: True)
    
    Returns:
        str: FAQ content
    """
    if use_cache:
        cache_key = question.lower().strip()
        if cache_key in _faq_cache:
            logger.info(f"Returning cached FAQ content", extra={
                "operation": "get_cached_faq",
                "trace_id": trace_id,
                "question": question[:50]
            })
            cached = _faq_cache[cache_key]
            return cached.replace("Trace ID: `{trace_id}`", f"Trace ID: `{trace_id}`")
    
    content = generate_faq_content(question, trace_id, repo)
    
    if use_cache:
        _faq_cache[cache_key] = content
    
    return content
